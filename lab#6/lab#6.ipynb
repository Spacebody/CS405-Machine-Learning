{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Yilin ZHENG\n",
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "train_dir = \"./train-mails/\"\n",
    "test_dir = \"./test-mails/\"\n",
    "\n",
    "# parameters\n",
    "most = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the data\n",
    "\n",
    "Ohhhh, it had been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(train_dir):\n",
    "    emails = glob.glob(os.path.join(train_dir, \"*\"))\n",
    "    all_words = []\n",
    "    for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for i, line in enumerate(m):\n",
    "                if i == 2:  # Body of the email\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list(list_to_remove):\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(most)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = make_dictionary(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mail_dir):\n",
    "    files = glob.glob(os.path.join(mail_dir, \"*\"))\n",
    "    feature_matrix = np.zeros((len(files), most))\n",
    "    doc_id = 0\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        word_id = 0\n",
    "                        for i, d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                word_id = i\n",
    "                                feature_matrix[doc_id, word_id] = words.count(word)\n",
    "        doc_id += 1\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matrix\n",
    "train_matrix = extract_features(train_dir)\n",
    "test_matrix = extract_features(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train labels\n",
    "train_set = glob.glob(os.path.join(train_dir, \"*\"))\n",
    "train_labels = [1 if data.find(\"spmsg\") == -1 else 0 for data in train_set]\n",
    "# get test labels\n",
    "test_set = glob.glob(os.path.join(test_dir, \"*\"))\n",
    "test_labels = [1 if data.find(\"spmsg\") == -1 else 0 for data in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn Bayes classifier for a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn Bayes classifier to compare my own implementation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score\n",
    "GNB = MultinomialNB() #MultinomialNB()\n",
    "GNB.fit(train_matrix, train_labels)\n",
    "y_pred = GNB.predict(test_matrix)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "recall = recall_score(test_labels, y_pred)\n",
    "f1 = f1_score(test_labels, y_pred)\n",
    "print(\"accuracy: {:05.2f}%, recall: {:05.2f}%, F-1: {:05.2f}%\" \\\n",
    "      .format(100 * accuracy, 100 * recall, 100 * f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My own implementation of Beyes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define my own multinomial Naïve Bayes classifier\n",
    "class Naive_Bayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, train_data, train_labels):\n",
    "        # train data, labels, and classes\n",
    "        self.data = train_data\n",
    "        self.labels = train_labels\n",
    "        self.feature_indices = [x for x in range(1, len(train_data[0])+1)]\n",
    "        # count positive and negative instances\n",
    "        self.pos_count = self.labels.count(1)\n",
    "        self.neg_count = self.labels.count(0)\n",
    "        self.total = len(train_labels)\n",
    "        self.pos = []\n",
    "        self.neg = []\n",
    "        for feature, label in zip(self.data, self.labels):\n",
    "            if label == 1:\n",
    "                self.pos.append(feature)\n",
    "            elif label == 0:\n",
    "                self.neg.append(feature)\n",
    "            else:\n",
    "                pass\n",
    "        # feature\n",
    "        self.features = {\"pos\": [], \"neg\": []}\n",
    "        # calculate prior\n",
    "        self.prior_pos = self.pos_count/self.total\n",
    "        self.prior_neg = self.neg_count/self.total\n",
    "        # calcuate probabilities of each feature for each class.\n",
    "        self.features[\"pos\"] = np.sum([[1 if feature[idx-1] > 0 else 0 for idx in self.feature_indices] \\\n",
    "                for feature in self.pos], axis=0)/self.pos_count\n",
    "        self.features[\"neg\"] = np.sum([[1 if feature[idx-1] > 0 else 0 for idx in self.feature_indices] \\\n",
    "                for feature in self.neg], axis=0)/self.neg_count\n",
    "\n",
    "    def predict(self, test_data, test_labels):\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        self.prediction = []\n",
    "        for data in self.test_data:\n",
    "            index_record = []\n",
    "            for index in self.feature_indices:\n",
    "                if data[index-1] > 0:\n",
    "                    index_record.append(index)\n",
    "            pos_prob = self.prior_pos * \\\n",
    "                    reduce(lambda x, y: x*y, [self.features[\"pos\"][index-1] for index in index_record])\n",
    "            neg_prob = self.prior_neg * \\\n",
    "                    reduce(lambda x, y: x*y, [self.features[\"neg\"][index-1] for index in index_record])\n",
    "            if pos_prob >= neg_prob:\n",
    "                self.prediction.append(1)\n",
    "            else:\n",
    "                self.prediction.append(0)\n",
    "        # calculate metrics\n",
    "        self.correct_pred = sum([1 if test == pred else 0 for test, pred in \\\n",
    "                                 zip(self.test_labels, self.prediction)])\n",
    "        self.TP = sum([1 if test == 1 and pred == 1 else 0 for test, pred in \\\n",
    "                       zip(self.test_labels, self.prediction)])\n",
    "        self.TN = sum([1 if test == 0 and pred == 0 else 0 for test, pred in \\\n",
    "                       zip(self.test_labels, self.prediction)])\n",
    "        self.FN = sum([1 if test == 1 and pred == 0 else 0 for test, pred in \\\n",
    "                       zip(self.test_labels, self.prediction)])\n",
    "        self.FP = sum([1 if test == 0 and pred == 1 else 0 for test, pred in \\\n",
    "                       zip(self.test_labels, self.prediction)])\n",
    "        return self.prediction\n",
    "    \n",
    "    def accuracy(self):\n",
    "        return self.correct_pred / len(self.test_labels)\n",
    "    \n",
    "    def recall(self):\n",
    "        return self.TP / (self.TP + self.FN)\n",
    "    \n",
    "    def F1(self):\n",
    "        return 2 * self.TP / (2 * self.TP + self.FP + self.FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 84.23%, recall: 99.23%, F-1: 86.29%\n"
     ]
    }
   ],
   "source": [
    "model = Naive_Bayes()\n",
    "model.fit(train_matrix, train_labels)\n",
    "y_pred2 = model.predict(test_matrix, test_labels)\n",
    "print(\"accuracy: {:05.2f}%, recall: {:05.2f}%, F-1: {:05.2f}%\" \\\n",
    "      .format(100 * model.accuracy(), 100 * model.recall(), 100 * model.F1()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
